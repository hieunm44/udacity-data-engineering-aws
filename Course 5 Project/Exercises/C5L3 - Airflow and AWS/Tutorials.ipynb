{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Airflow and AWS",
   "id": "ae47725b14ad0dd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "1. Tạo IAM user -> Cấp policy Admin, Redshift, S3 -> Tạo Access key\n",
    "2. Trong Airflow UI, tạo connection với AWS (dùng Access key đã tạo)\n",
    "3. Tạo S3 bucket và copy data vào từ S3 bucket của Udacity\n",
    "4. Trong DAG, dùng S3Hook để kết nối với S3 bucket và log files\n",
    "5. Tạo IAM role, cấp policy Redshift, S3\n",
    "6. Tạo Redshift Serverless (dùng IAM role vừa tạo)\n",
    "7. Trong Airflow UI, tạo connection với Redshift\n",
    "8. Trong DAG, dùng PostgresHook để kết nối với Redshift -> Tạo tables\n",
    "9. Vào Redshift workgroup để query data"
   ],
   "id": "dbb7a9908f573e42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Tạo một IAM user trên AWS với 3 policies: `AdministratorAccess`, `AmazonRedshiftFullAccess`, `AmazonS3FullAccess`. Tạo Access key và lưu lại.\n",
    "2. Trong Airflow, vào **Admin** -> **Connections**. Tạo connection với AWS như ở dưới. \\\n",
    "   (Nhớ install package `apache-airflow-providers-amazon` bản mới nhất và đặt `test_connection = Enabled` trong file `airflow.cfg`)\n",
    "\n",
    "   <img src=\"images/aws1.png\" width=1500>\n",
    "\n",
    "3. Tạo S3 bucket và copy data vào\n",
    "   ```bash\n",
    "   aws s3 mb s3://nd027-hieu\n",
    "   aws s3 cp s3://udacity-dend/data-pipelines/ ~/data-pipelines/ --recursive\n",
    "   aws s3 cp ~/data-pipelines/ s3://nd027-hieu/data-pipelines/ --recursive\n",
    "   aws s3 ls s3://nd027-hieu/data-pipelines/\n",
    "   ```\n",
    "\n",
    "4. Trong Airflow, vào **Admin** -> **Variables**. Tạo các variables như ở dưới:\n",
    "\n",
    "   <img src=\"images/aws2.png\" width=1500>\n",
    "\n",
    "5. Xem hai files `ex1_sql_statements.py` và `ex2_connections_hooks.py`. Nhớ sửa lại tên S3 bucket. Chạy file `ex2_connections_hooks.py` trên Airflow, sẽ log các files ở trong S3 bucket trên. Chạy xong vào xem log file sẽ thấy."
   ],
   "id": "e678fc2612f59024"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "6. Tạo Redshift Role trên AWS\n",
    "   ```bash\n",
    "   aws iam create-role --role-name my-redshift-service-role --assume-role-policy-document '{\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"redshift.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }'\n",
    "   ```\n",
    "\n",
    "7. Cấp full access to S3 cho role vừa tạo\n",
    "   ```bash\n",
    "   aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --role-name my-redshift-service-role\n",
    "   ```\n",
    "\n",
    "8. Vào **Redshift** -> **Redshift Serverless** \\\n",
    "   Chọn **Customize settings**, điền admin và password. \\\n",
    "   Trong **Associated IAM roles**, Chọn **Associate IAM roles**, chọn `my-redshift-service-role`.\n",
    "   Tick **Turn on enhanced VPC routing**, còn lại để nguyên -> **Save configuration** \\\n",
    "   Vào Workgroup configuration để kiểm tra.\n",
    "\n",
    "   <img src=\"images/aws3.png\" width=1500>\n",
    "\n",
    "9. Ấn vào Workgroup vừa tạo. Trong phần **Network and security**, chọn **Edit**.\n",
    "\n",
    "   <img src=\"images/aws4.png\" width=1500>\n",
    "\n",
    "10. Tick **Turn on Publicly accessible** -> **Save changes**\n",
    "\n",
    "    <img src=\"images/aws5.png\" width=700>\n",
    "\n",
    "11. Ấn vào link ớ dưới **VPC security group**, sẽ đc đưa tới Security Groups trong EC2. \\\n",
    "    Ấn vào **Security group**, thêm một **Inboud rule**.\n",
    "\n",
    "    <img src=\"images/aws6.png\" width=1500>\n",
    "\n",
    "12. Tạo connection với Redshift trong Airflow\n",
    "\n",
    "    <img src=\"images/aws7.png\" width=1500>"
   ],
   "id": "b4237b7ae6beb199"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "13. Xem file `ex3_s3_to_redshift.py`. Chạy file này trên Airflow, sẽ tạo một table `trips` trong Redshift -> Load data vào đó từ file `divvy_trips_2018.csv` trong S3 -> Tạo table `station_traffic`. \\\n",
    "    Lưu ý: Để import trực tiếp code từ một file khác (ở đây có `import sql_statement`), ko đc để các files trong subfolder ở folder `home/airflow/dags`. Nếu muốn để file trong subfolder, phải đặt tên subfolder ko có space, VD `c5l3`, và dùng `from c5l3 import sql_statements`.\n",
    "14. Vào worksgroup trên Redshift, ấn vào **Query data**, sẽ đc đưa tới **Query editor** trong Redshift.\n",
    "\n",
    "    <img src=\"images/aws8.png\" width=1500>\n",
    "\n",
    "15. Ấn vào dấu $>$ bên cạnh tên workgroup để kết nối tới DB.\n",
    "\n",
    "    <img src=\"images/aws9.png\" width=700>\n",
    "\n",
    "16. Để nguyên, ấn **Create connection**.\n",
    "\n",
    "    <img src=\"images/aws10.png\" width=400>\n",
    "\n",
    "17. Vào xem các tables đã tạo. Ấn chuột phải vào table \"station_traffic\" -> **Select table** -> **Run**\n",
    "\n",
    "    <img src=\"images/aws11.png\" width=1500>\n"
   ],
   "id": "dea31519a3e99cc4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
